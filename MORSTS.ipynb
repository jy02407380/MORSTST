{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import LSTM\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from random import choice\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "# from msvr import kernelmatrix\n",
    "# from msvr import msvr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MSVR_F:\n",
    "#     def __init__(self,ker  = 'rbf', C = 2,epsi = 0.001,par  = 0.8,tol  = 1e-10):\n",
    "#         self.ker = ker\n",
    "#         self.C = C\n",
    "#         self.epsi = epsi\n",
    "#         self.par = par\n",
    "#         self.tol = tol\n",
    "        \n",
    "#     def fit(self,X_train,Y_train):\n",
    "#         X_train = np.array(X_train)\n",
    "#         Y_train = np.array(Y_train)\n",
    "#         self.X_train = X_train\n",
    "#         self.Beta = msvr(X_train, Y_train, self.ker, self.C, self.epsi, self.par, self.tol)\n",
    "    \n",
    "#     def predict(self,X_test):\n",
    "#         X_test = np.array(X_test)\n",
    "#         H = kernelmatrix('rbf', X_test, self.X_train, par);\n",
    "#         return np.dot(H, self.Beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire un générateur de blocs pour obtenir différents ensembles de données initiales\n",
    "# data est la donnée, n est le nombre de parties en lesquelles elle est divisée, i est la première\n",
    "# 写一个块生成器，用来获得不同的初始数据集\n",
    "# data 是数据，n是分成几份，i是第几份\n",
    "\n",
    "def getChunks(data,n,i):\n",
    "    return data[ int(len(data)*(i)/n)+1 : (int(len(data)*(i+1)/n)-1)]\n",
    "def getPourCentage(data,beginCentage,endCentage):\n",
    "    return data[ (int(len(data)*beginCentage)): (int(len(data)*endCentage))]\n",
    "#功能介绍：返回初始数据，返回全部训练数据，判断是否还有下一个batch，返回当前batch，\n",
    "#Description de la fonction : retour des données initiales, retour de toutes les données d'entraînement, détermination de l'existence d'un lot suivant, retour du lot actuel.\n",
    "class feeder_Ini_Train_Batch():\n",
    "    def __init__(self,X,Y,beginCentage,endCentage,batch_size):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        #elf.X_train = getPourCentage(X,0,iniCentage)\n",
    "        #elf.Y_train = getPourCentage(Y,0,iniCentage)\n",
    "        self.X_test = getPourCentage(X,beginCentage,endCentage)\n",
    "        self.Y_test = getPourCentage(Y,beginCentage,endCentage)\n",
    "        self.batch_size = batch_size\n",
    "        self.t = 1 # t C'est l'indexation et le temps. 就是索引和时间\n",
    "    def getIni_X_Y(self,iniCentage):\n",
    "        return getPourCentage(self.X,0,iniCentage),getPourCentage(self.Y,0,iniCentage)\n",
    "    def getTrain_X_Y(self):\n",
    "        return self.X_test,self.Y_test\n",
    "    def hasThisBatch(self):\n",
    "        if (self.t)*self.batch_size < len (self.X_test):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def hasThisBatch_and_nextBath(self):\n",
    "        if (self.t+1)*(self.batch_size) < len (self.X_test):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def getThisBatch(self):\n",
    "        if self.hasThisBatch() == True:\n",
    "            actul_Batch_X = self.X_test[(self.t-1)*self.batch_size:(self.t)*self.batch_size]\n",
    "            actul_Batch_Y = self.Y_test[(self.t-1)*self.batch_size:(self.t)*self.batch_size]\n",
    "            return actul_Batch_X,actul_Batch_Y\n",
    "        else:\n",
    "            print('err index out')\n",
    "    def getNextBatch_getThisBatch(self):\n",
    "        if self.hasThisBatch() == True:\n",
    "            next_Batch_X = self.X_test[(self.t)*self.batch_size:(self.t+1)*self.batch_size]\n",
    "            next_Batch_Y = self.Y_test[(self.t)*self.batch_size:(self.t+1)*self.batch_size]\n",
    "            return next_Batch_X,next_Batch_Y\n",
    "        else:\n",
    "            print('err index out')\n",
    "    def goNext(self):\n",
    "        self.t +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanSuperPourCentage(data,PourCentage): # Calculez la valeur moyenne du pourcentage supérieur des données. 计算排名前百分之多少的数据的均值\n",
    "#     print(np.percentile(data, (PourCentage)))\n",
    "    return np.mean([x for x in data if x >= np.percentile(data, (PourCentage))])\n",
    "def getMeanSuperPourCentage_Martix_ParLine(data,PourCentage):\n",
    "    return np.array([getMeanSuperPourCentage(x,PourCentage) for x in data ])\n",
    "\n",
    " # 0子模型 1编号时间 2权重 3class名 4俗称 5varepsilon加权错  6 Omega时间权重  7err普通RMSE错误  8 模型地址\n",
    " # 0 submodèle 1 temps de numérotation 2 poids 3 nom de classe 4 nom commun 5 erreur de pondération varepsilon 6 pondération temps oméga 7 erreur RMSE commune 8 adresse du modèle\n",
    "\n",
    "def RandonSelectionModle(index,x,y , optitionInnErrOrCroissErr,numFlod = 3, indique_sousModle = 'Random_LS_LI_R_T'):  \n",
    "\n",
    "    if indique_sousModle == 'Random_LI_R_T_KNN':\n",
    "        indique_sousModle = choice(['Lasso','R_Forest','Tree','KNN'])\n",
    "#     elif indique_sousModle == 'Random_LS_LI_R_T_KNN':\n",
    "#         indique_sousModle = choice(['LSTM','Lasso','R_Forest','Tree','KNN'])\n",
    "#     elif indique_sousModle == 'Random_LS_LI_R_T_SVR':\n",
    "#         indique_sousModle = choice(['LSTM','Lasso','R_Forest','Tree','SVR'])\n",
    "#     elif indique_sousModle == 'Random_LS_LI_R_T':\n",
    "#         indique_sousModle = choice(['LSTM','Lasso','R_Forest','Tree'])\n",
    "#     elif indique_sousModle == 'Random_LS_LI_R_KNN':\n",
    "#         indique_sousModle = choice(['LSTM','Lasso','R_Forest','KNN'])\n",
    "#     elif indique_sousModle == 'Random_LS_LI_T_KNN':\n",
    "#         indique_sousModle = choice(['LSTM','Lasso','Tree','KNN'])\n",
    "#     elif indique_sousModle == 'Random_LS_R_T_KNN':\n",
    "#         indique_sousModle = choice(['LSTM','R_Forest','Tree','KNN'])\n",
    "#     elif indique_sousModle == 'Random_LS_R_T':\n",
    "#         indique_sousModle = choice(['LSTM','R_Forest','Tree'])\n",
    "#     elif indique_sousModle == 'Random_LS_LI_T':\n",
    "#         indique_sousModle = choice(['LSTM','Lasso','Tree'])\n",
    "        \n",
    "        print('Randomly selected indique_sousModle ',indique_sousModle)\n",
    "    weight = 1 # weight of  this base model 投票权重\n",
    "    varepsilon = 1 # Résultats des fonctions de perte 丢失函数结果 也就是 varepsilon 加权之后的错\n",
    "    omega = 1 #  la pondération du temps 关于时间的权重\n",
    "    RMSE = 1 # err 普通RMSE错误\n",
    "    x_rnn = np.reshape(x, (x.shape[0], 1,x.shape[1]))\n",
    "    kf = KFold(n_splits=numFlod)\n",
    "    yhat_all_flod = []\n",
    "    \n",
    "\n",
    "    \n",
    "#     elif indique_sousModle == 'LSTM' :\n",
    "#         modele = Sequential()\n",
    "#         modele.add(LSTM(500, input_shape =(1, x_rnn.shape[2]) , activation='relu'))\n",
    "#         modele.add(Dense(500, activation='relu'))\n",
    "#         modele.add(Dense(y.shape[1] , activation='sigmoid'))\n",
    "#         # Compile model\n",
    "#         modele.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#         modele.fit(x_rnn, y, epochs=6, batch_size=2000,  verbose=2)\n",
    "#         if optitionInnErrOrCroissErr == 'CroissErr':\n",
    "#             for train, test in kf.split(x_rnn):\n",
    "#                 X_train_numFlod = np.array( [x_rnn[i] for i in train])\n",
    "#                 Y_train_numFlod = np.array([y[i] for i in train])\n",
    "#                 X_test_numFlod = np.array([x_rnn[i] for i in test])\n",
    "#                 Y_test_numFlod = np.array([y[i] for i in test])\n",
    "#                 modele_numFlod = Sequential()\n",
    "#                 modele_numFlod.add(LSTM(500, input_shape =(1, x_rnn.shape[2]) , activation='relu'))\n",
    "#                 modele_numFlod.add(Dense(500, activation='relu'))\n",
    "#                 modele_numFlod.add(Dense(y.shape[1] , activation='sigmoid'))\n",
    "#                 modele_numFlod.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#                 modele_numFlod.fit(X_train_numFlod, Y_train_numFlod, epochs=4, batch_size=1000,  verbose=2)\n",
    "#                 yhat__numFlod = modele_numFlod.predict(X_test_numFlod)\n",
    "#                 yhat_all_flod += yhat__numFlod.tolist()\n",
    "#         return [modele,index,weight,modele.__class__,'LSTM',varepsilon,omega,RMSE,id(modele)] , yhat_all_flod\n",
    "    \n",
    "    \n",
    "    \n",
    "#     elif indique_sousModle == 'SVR' :\n",
    "#         modele = MSVR_F()\n",
    "#         modele.fit(x,y)\n",
    "#         if optitionInnErrOrCroissErr == 'CroissErr':\n",
    "#             for train, test in kf.split(x):\n",
    "#                 X_train_numFlod = np.array( [x[i] for i in train])\n",
    "#                 Y_train_numFlod = np.array([y[i] for i in train])\n",
    "#                 X_test_numFlod = np.array([x[i] for i in test])\n",
    "#                 Y_test_numFlod = np.array([y[i] for i in test])\n",
    "#                 modele_numFlod = MSVR_F()\n",
    "#                 modele_numFlod.fit(X_train_numFlod,Y_train_numFlod)\n",
    "#                 yhat__numFlod = modele_numFlod.predict(X_test_numFlod)\n",
    "#                 yhat_all_flod += yhat__numFlod.tolist()        \n",
    "#         return [modele,index,weight,modele.__class__,'SVR',varepsilon,omega,RMSE,id(modele)],yhat_all_flod\n",
    "    \n",
    "\n",
    "    if indique_sousModle == 'KNN':\n",
    "        modele = KNeighborsRegressor(n_neighbors=15)\n",
    "        modele.fit(x,y)\n",
    "        if optitionInnErrOrCroissErr == 'CroissErr':\n",
    "            for train, test in kf.split(x):\n",
    "                X_train_numFlod = np.array( [x[i] for i in train])\n",
    "                Y_train_numFlod = np.array([y[i] for i in train])\n",
    "                X_test_numFlod = np.array([x[i] for i in test])\n",
    "                Y_test_numFlod = np.array([y[i] for i in test])\n",
    "                modele_numFlod = KNeighborsRegressor(n_neighbors=15)\n",
    "                modele_numFlod.fit(X_train_numFlod,Y_train_numFlod)\n",
    "                yhat__numFlod = modele_numFlod.predict(X_test_numFlod)\n",
    "                yhat_all_flod += yhat__numFlod.tolist()        \n",
    "        return [modele,index,weight,modele.__class__,'KNeighborsRegressor',varepsilon,omega,RMSE,id(modele)],yhat_all_flod\n",
    "    \n",
    "    elif indique_sousModle == 'R_Forest':\n",
    "        modele = RandomForestRegressor(n_estimators=10, )\n",
    "        modele.fit(x,y)\n",
    "        if optitionInnErrOrCroissErr == 'CroissErr':\n",
    "            for train, test in kf.split(x):\n",
    "                X_train_numFlod = np.array( [x[i] for i in train])\n",
    "                Y_train_numFlod = np.array([y[i] for i in train])\n",
    "                X_test_numFlod = np.array([x[i] for i in test])\n",
    "                Y_test_numFlod = np.array([y[i] for i in test])\n",
    "                modele_numFlod = RandomForestRegressor(n_estimators=10,)\n",
    "                modele_numFlod.fit(X_train_numFlod,Y_train_numFlod)\n",
    "                yhat__numFlod = modele_numFlod.predict(X_test_numFlod)\n",
    "                yhat_all_flod += yhat__numFlod.tolist()\n",
    "        return [modele,index,weight,modele.__class__,'RandomForestRegressor',varepsilon,omega,RMSE,id(modele)],yhat_all_flod\n",
    "            \n",
    "    elif indique_sousModle == 'Lasso':\n",
    "        modele = Lasso(alpha=0.1)\n",
    "        modele.fit(x,y)\n",
    "        if optitionInnErrOrCroissErr == 'CroissErr':\n",
    "            for train, test in kf.split(x):\n",
    "                X_train_numFlod = np.array( [x[i] for i in train])\n",
    "                Y_train_numFlod = np.array([y[i] for i in train])\n",
    "                X_test_numFlod = np.array([x[i] for i in test])\n",
    "                Y_test_numFlod = np.array([y[i] for i in test])\n",
    "                modele_numFlod = Lasso(alpha=0.1)\n",
    "                modele_numFlod.fit(X_train_numFlod,Y_train_numFlod)\n",
    "                yhat__numFlod = modele_numFlod.predict(X_test_numFlod)\n",
    "                yhat_all_flod += yhat__numFlod.tolist()        \n",
    "        return [modele,index,weight,modele.__class__,'LassoRegression',varepsilon,omega,RMSE,id(modele)],yhat_all_flod\n",
    "        \n",
    "    elif indique_sousModle == 'Tree':\n",
    "        modele = DecisionTreeRegressor()\n",
    "        modele.fit(x,y)\n",
    "        if optitionInnErrOrCroissErr == 'CroissErr':\n",
    "            for train, test in kf.split(x):\n",
    "                X_train_numFlod = np.array( [x[i] for i in train])\n",
    "                Y_train_numFlod = np.array([y[i] for i in train])\n",
    "                X_test_numFlod = np.array([x[i] for i in test])\n",
    "                Y_test_numFlod = np.array([y[i] for i in test])\n",
    "                modele_numFlod = DecisionTreeRegressor()\n",
    "                modele_numFlod.fit(X_train_numFlod,Y_train_numFlod)\n",
    "                yhat__numFlod = modele_numFlod.predict(X_test_numFlod)\n",
    "                yhat_all_flod += yhat__numFlod.tolist()        \n",
    "        return [modele,index,weight,modele.__class__,'DecisionTreeRegressor',varepsilon,omega,RMSE,id(modele)],yhat_all_flod\n",
    "        \n",
    "def makePredictionModele(modele,x):\n",
    "    if type(modele) in [type(RandomForestRegressor()), \n",
    "                          type(KNeighborsRegressor()),\n",
    "                          type(Lasso()),\n",
    "                          type(DecisionTreeRegressor())] :\n",
    "#                                   type(MSVR_F())\n",
    "\n",
    "        return  modele.predict(x)\n",
    "    else:\n",
    "        print('报错，不在列表中 Error reported, not in the list')\n",
    "\n",
    "def beta_fonction(k,t,a,b):  \n",
    "    return 1/(1+ math.exp(-a*(t-k-b))) \n",
    "\n",
    "def vote_mean(dic_expert, actul_Batch_X, actul_Batch_Y, afficher_detail = False):\n",
    "    H_res = np.array( [[0.0]* len(actul_Batch_Y[0])] *len(actul_Batch_Y))\n",
    "    print(H_res)\n",
    "    sumWeigt = 0\n",
    "    \n",
    "    list_RMSE_SousModele = []\n",
    "\n",
    "    for key,value in dic_expert.items():\n",
    "        yhat_sousM = makePredictionModele(value[0],actul_Batch_X)\n",
    "        \n",
    "        if np.array(yhat_sousM).ndim == 1:\n",
    "            print('type([[z] for z in yhat_sousM])  ',type([[z] for z in yhat_sousM]))\n",
    "            yhat_sousModel = np.array([[z] for z in yhat_sousM])\n",
    "        else:\n",
    "            yhat_sousModel = yhat_sousM\n",
    "            print(type(yhat_sousModel))\n",
    "        print('Output of each sub-model',yhat_sousModel)\n",
    "        H_res =H_res + yhat_sousModel*value[2]\n",
    "        sumWeigt += value[2] #  2 représente les poids des votes des sous-modèles 2号代表子模型投票权\n",
    "        list_RMSE_SousModele.append(aRMSE( actul_Batch_Y ,yhat_sousModel))\n",
    "    yhat_H = np.array(H_res)/sumWeigt # L'obtention des résultats prédits pour H\n",
    "    \n",
    "#     print('投票结束之后的输出', yhat_H)\n",
    "#     print('Y的原始值', actul_Batch_Y)\n",
    "    if afficher_detail == True:\n",
    "        print( 'list_RMSE_SousModele ', list_RMSE_SousModele, 'mean of list_RMSE_SousModele', np.mean(list_RMSE_SousModele) )\n",
    "        print(' RMSE resultat vote', aRMSE( actul_Batch_Y ,yhat_H))\n",
    "    return yhat_H\n",
    "\n",
    "\n",
    "def vote_OnlyMaxERRWeight(dic_expert, actul_Batch_X, actul_Batch_Y):\n",
    "    the_key = 0\n",
    "    maxWeight = 0\n",
    "    for key,value in dic_expert.items():\n",
    "        if dic_expert[key][5] > maxWeight:\n",
    "            maxWeight = dic_expert[key][5]\n",
    "            the_key = key\n",
    "    yhat_H = makePredictionModele(dic_expert[the_key][0],actul_Batch_X)\n",
    "    err_H =  aRMSE(actul_Batch_Y,yhat_H)# err as RMSE \n",
    "    print('Sub-model selected to vote, no. ', the_key, ' 其 RMSE', err_H)\n",
    "    return yhat_H\n",
    "\n",
    "def vote_OnlyMaxESpWeight(dic_expert, actul_Batch_X, actul_Batch_Y):\n",
    "    the_key = 0\n",
    "    maxWeight = 0\n",
    "    for key,value in dic_expert.items():\n",
    "        if dic_expert[key][2] > maxWeight:\n",
    "            maxWeight = dic_expert[key][2]\n",
    "            the_key = key\n",
    "    yhat_H = makePredictionModele(dic_expert[the_key][0],actul_Batch_X)\n",
    "    err_H =  aRMSE(actul_Batch_Y,yhat_H)# err as RMSE \n",
    "    print('Sub-model selected to vote, no. ', the_key, ' with its RMSE', err_H)\n",
    "    return yhat_H\n",
    "\n",
    "def updatingALLSousModele(dic_expert, x, y):\n",
    "    print (' Update sub-model program start' )\n",
    "    x_rnn = np.reshape(x, (x.shape[0], 1,x.shape[1]))\n",
    "    for key,value in dic_expert.items():\n",
    "        if type(value[0]) in [type(Sequential())]:\n",
    "            value[0].fit(x_rnn, actul_Batch_Y)\n",
    "\n",
    "            \n",
    "def get_filename(path,filetype):  # 输入路径、文件类型例如'.csv'\n",
    "    name = []\n",
    "    for root,dirs,files in os.walk(path):\n",
    "        for i in files:\n",
    "            if os.path.splitext(i)[1]==filetype:\n",
    "                name.append(i)    \n",
    "    return name\n",
    "\n",
    "def aRMSE(y_true,y_pred):\n",
    "    return np.mean(mean_squared_error(y_true, y_pred, squared=False, multioutput='raw_values'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path_X_Y_data = 'simulationDonnes/'\n",
    "# Y = np.load(path_X_Y_data + 'Y_CU_BEMS_19_normale_nettoy.npy' ,allow_pickle=True)\n",
    "# X = np.load(path_X_Y_data + 'X_CU_BEMS_19_normale_nettoybackTimePoint3PCA10.npy' ,allow_pickle=True)\n",
    "\n",
    "Y = np.load(path_X_Y_data + 'Y(2ans)Norma_futureTimePoint0.npy' ,allow_pickle=True)\n",
    "X = np.load(path_X_Y_data + 'X(2ans)Norma_backTimePoint5.npy' ,allow_pickle=True)\n",
    "\n",
    "        \n",
    "while(True):\n",
    "    \n",
    "    dic_expert = {} #Conserver un dictionnaire des sous-modèles\n",
    "    timeList, errList, innErrList, testErrNextBatchList,nFlodErr= [],[],[],[],[] #Liste des temps, liste de la précision \n",
    "    Resulta_list, Ture_list = [],[] #Recorded information: predicted results and true values\n",
    "    dic_expert = {} #Conserver un dictionnaire des sous-modèles\n",
    "    ini_pourCentage = 0.01 # Le rapport initial doit être défini \n",
    "    testStar,testEnd = 0.2,0.99\n",
    "    HowManyDaysForBatch =14\n",
    "    batch_size = 1400*HowManyDaysForBatch# Taille du bloc\n",
    "    a,b = 0.35,1\n",
    "    time_Ini = 0 # Le temps nécessaire à l'initialisation est enregistré ici\n",
    "    varepsilon_extreme_bord = 1.2 # Coefficient permettant de vérifier si le sous-modèle fonctionne correctement\n",
    "    maxNumSousModle = 10 # Nombre maximal de sous-modèles\n",
    "    testBatchNum = 100 #Contrôler la quantité de circulation pendant l'expérience\n",
    "    Q3 = 75 # Définir les coefficients pour le calcul des poids d'instance\n",
    "    optionPCA, PCA_size = True, 25\n",
    "#     optitionInnErrOrCroissErr = 'InnErr'\n",
    "    optitionInnErrOrCroissErr = 'CroissErr'\n",
    "    optitionAddSousModele = True\n",
    "    optionVote ='Mean'\n",
    "    # optionVote ='OnlyMaxERRWeight'\n",
    "    # optionVote = 'vote_OnlyMaxESpWeight'\n",
    "    indicateur_sousModel = 'Random_LI_R_T_KNN' #['Random','LSTM','Lasso','KNN','R_Forest'] # 0 随机，1 LSTM，\n",
    "    cStepAugment = 1.4\n",
    "    updateOrNon = False\n",
    "\n",
    "\n",
    "    if optionPCA == True:\n",
    "        pca = PCA(n_components=PCA_size)\n",
    "        principalComponents = pca.fit_transform(X)\n",
    "        pca.explained_variance_ratio_\n",
    "        importance = pca.explained_variance_ratio_\n",
    "        plt.scatter(range(1,PCA_size+1),importance)\n",
    "        plt.plot(range(1,PCA_size+1),importance)\n",
    "        plt.title('Scree Plot')\n",
    "        plt.xlabel('Factors')\n",
    "        plt.ylabel('Eigenvalue')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        X = pca.fit_transform(X)\n",
    "    \n",
    "    myFeeder = feeder_Ini_Train_Batch(X,Y,testStar,testEnd,batch_size) # X,Y beginCentage,endCentage,batch_size \n",
    "    X_train,Y_train = myFeeder.getIni_X_Y(ini_pourCentage) #这里才设定初始比例\n",
    "    X_test,Y_test = myFeeder.getTrain_X_Y()\n",
    "\n",
    "    iniFlage = True\n",
    "\n",
    "    while ((testBatchNum > myFeeder.t)  and myFeeder.hasThisBatch()):\n",
    "\n",
    "        # Créer un sous-modèle initialisé, code 0\n",
    "        start_Ini = time.process_time()# Heure de début d'enregistrement记录开始时间\n",
    "        if iniFlage == True:\n",
    "            Delta = [1/ myFeeder.batch_size]* myFeeder.batch_size #Valeur par défaut de delta delta的默认值 \n",
    "            dic_expert[0], err_numFlod= RandonSelectionModle(0,X_train, Y_train,optitionInnErrOrCroissErr, indique_sousModle = indicateur_sousModel)\n",
    "            iniFlage = False\n",
    "        time_Ini = round(time.process_time()-start_Ini,5) # Le temps nécessaire à l'initialisation est enregistré ici 这里记录了初始化需要的时间\n",
    "        start = time.process_time()# Heure de début d'enregistrement 记录开始时\n",
    "        \n",
    "        #----------------VOTE------------------------------------------------------\n",
    "        actul_Batch_X, actul_Batch_Y  =  myFeeder.getThisBatch()\n",
    "        # Calculer la performance de Ht-1 à t \n",
    "        if optionVote == 'Mean':\n",
    "            yhat_H = vote_mean(dic_expert,actul_Batch_X, actul_Batch_Y,afficher_detail=True)\n",
    "        elif optionVote == 'Max':\n",
    "            yhat_H = vote_OnlyMaxWeight(dic_expert,actul_Batch_X, actul_Batch_Y)\n",
    "\n",
    "        print('vote is over')\n",
    "        #----------------------------------------------------------------------\n",
    "\n",
    "        #----------------Confirm instance weights and record current cluster performance------------------------------------------------------\n",
    "        ERR_absolu_H = np.abs(actul_Batch_Y - yhat_H) #ERR_absolu (martix)\n",
    "        Resulta_list.append(yhat_H)\n",
    "        Ture_list.append(actul_Batch_Y)\n",
    "        err_H = aRMSE(actul_Batch_Y,yhat_H)# err as RMSE \n",
    "        meanERR_absolu_H = np.mean(ERR_absolu_H, axis=1)\n",
    "    #     Delta = getMeanSuperPourCentage_Martix_ParLine(ERR_absolu_H,Q3)\n",
    "    #     varepsilon_H = np.average(meanERR_absolu_H,weights=Delta)\n",
    "\n",
    "        varepsilon_H = np.average(meanERR_absolu_H)\n",
    "        errList.append(err_H) \n",
    "        print('Instance weights, recording current group performance, over')\n",
    "        #----------------确认实例权重，记录当前群性能------------------------------------------------------\n",
    "\n",
    "        #----------------Alarm in case of abnormality------------------------------------------------------\n",
    "        if err_H >10:\n",
    "            print('time: ',myFeeder.t,'Une erreur majeure s est produite：',err_H) \n",
    "        #----------------当群性能异常时，报警------------------------------------------------------\n",
    "\n",
    "\n",
    "        #------------------Add a new sub-model and confirm its weights (from cross-validation)-----------------------------\n",
    "        # 添加一个新的子模型    \n",
    "        # 这里有新改动，未在     meanERR_absolu < 0.1\n",
    "        bord = 1 \n",
    "        while(optitionAddSousModele):\n",
    "            print('Start adding new sub-models')\n",
    "            dic_expert[myFeeder.t], yhat_new_numFlod = RandonSelectionModle(myFeeder.t, actul_Batch_X, actul_Batch_Y, optitionInnErrOrCroissErr,indique_sousModle = indicateur_sousModel)\n",
    "\n",
    "        \n",
    "            if optitionInnErrOrCroissErr == 'CroissErr':\n",
    "                ERR_absolu_new_numFlod = np.abs(actul_Batch_Y - yhat_new_numFlod) \n",
    "                meanERR_absolu_new_numFlod = np.mean(ERR_absolu_new_numFlod, axis=1)\n",
    "                err_new_numFlod = aRMSE(actul_Batch_Y,yhat_new_numFlod) \n",
    "                varepsilon_new = np.average(meanERR_absolu_new_numFlod,weights=Delta)\n",
    "                if varepsilon_new < varepsilon_H*bord:\n",
    "                    nFlodErr.append(err_new_numFlod)\n",
    "                    break\n",
    "                else:\n",
    "                    bord = bord*cStepAugment\n",
    "                \n",
    "            elif optitionInnErrOrCroissErr == 'InnErr':\n",
    "\n",
    "                yhat_new_inner = makePredictionModele(dic_expert[myFeeder.t][0], actul_Batch_X)\n",
    "                ERR_absolu_new_inner = np.abs(actul_Batch_Y - yhat_new_inner) \n",
    "                meanERR_absolu_new_inner = np.mean(ERR_absolu_new_inner, axis=1)\n",
    "                err_new_inner = aRMSE(actul_Batch_Y,yhat_new_inner) \n",
    "                varepsilon_new = np.average(meanERR_absolu_new_inner,weights=Delta)\n",
    "                break\n",
    "        #------------------添加一个新的子模型 ，确认其权重（来自交叉验证）-----------------------------\n",
    "        \n",
    "        if optitionAddSousModele:\n",
    "            #------------------Recording training errors for new sub-models-----------------------------\n",
    "            yhat_new_inner = makePredictionModele(dic_expert[myFeeder.t][0], actul_Batch_X)\n",
    "            err_new_inner = aRMSE(actul_Batch_Y,yhat_new_inner)         \n",
    "            innErrList.append(err_new_inner)\n",
    "            #------------------对新的子模型记录训练错误-----------------------------\n",
    "\n",
    "            #------------------Test error of the model generated by the previous block in the next block-----------------------------\n",
    "            if myFeeder.hasThisBatch_and_nextBath():\n",
    "                next_batch_test_X,  next_batch_test_Y =  myFeeder.getNextBatch_getThisBatch()\n",
    "                yhat_next_batch_test = makePredictionModele(dic_expert[myFeeder.t][0], next_batch_test_X)\n",
    "                err_test_NextBatch = aRMSE(next_batch_test_Y, yhat_next_batch_test)         \n",
    "                testErrNextBatchList.append(err_test_NextBatch)\n",
    "            #------------------对上一个块的子模型在下一块的测试错误-----------------------------\n",
    "\n",
    "\n",
    "        #------------------Calculate the performance of the old sub-model based on the weighted strengths in the current block-----------\n",
    "        Varepsilon_list = [] #Collecte de la liste des pesées séparées de Varepsilon pour faciliter la recherche des valeurs max-min. 收集Varepsilon单独称list，方便找最大最小值\n",
    "        for key,value in dic_expert.items():\n",
    "            yhat_oldModel = makePredictionModele(value[0], actul_Batch_X)\n",
    "            ERR_absolu_oldModel = np.abs(actul_Batch_Y - yhat_oldModel)\n",
    "            mean_ERR_absolu_oldModel = np.mean(ERR_absolu_oldModel, axis=1)\n",
    "            err_oldModel = aRMSE(actul_Batch_Y,yhat_oldModel) # RMSE\n",
    "            varepsilon_oldModel = np.average(mean_ERR_absolu_oldModel,weights=Delta)\n",
    "    #         if varepsilon_oldModel > varepsilon_H*varepsilon_extreme_bord: \n",
    "    #             value[5] = varepsilon_H*varepsilon_extreme_bord\n",
    "    #         else:\n",
    "    #             value[5] = varepsilon_oldModel\n",
    "            value[5] = varepsilon_oldModel\n",
    "            Varepsilon_list.append(value[5])\n",
    "            value[7] = err_oldModel # enregrister RMSE\n",
    "    #         for key,value in dic_expert.items():   #normalisation \n",
    "    #             value[5] = (value[5] - min(Varepsilon_list))/(max(Varepsilon_list) - min(Varepsilon_list))\n",
    "    #     for key,value in dic_expert.items(): \n",
    "    #         print('key ', key, ' value[5] ', value[5] )\n",
    "        #------------------根据当前块中被加权的实力，计算旧的子模型的表现（7号普通错误），并更新权重（5号根据加权错误）-----------\n",
    "\n",
    "\n",
    "        #------------------Calculating time weights ----------\n",
    "        for key,value in dic_expert.items(): \n",
    "            numerator = beta_fonction(value[1],myFeeder.t,a,b)\n",
    "            denominator = 0\n",
    "            for j in range(0, myFeeder.t - value[1] +1): # \n",
    "                denominator +=  beta_fonction(myFeeder.t - j,myFeeder.t,a,b)\n",
    "#             print('denominator  ' , denominator)\n",
    "#             if  denominator == 0:\n",
    "#                 print(' myFeeder.t  ', myFeeder.t,' value[1]', value[1] )\n",
    "#                 for j in range(0, myFeeder.t - value[1] +1): # \n",
    "#                     print('j :', j , ' beta_fonction(myFeeder.t - j,myFeeder.t,a,b)', beta_fonction(myFeeder.t - j,myFeeder.t,a,b))\n",
    "            value[6] =  numerator/denominator\n",
    "        for key,value in dic_expert.items(): \n",
    "            print('key ', key, ' value[6] ', value[6] )\n",
    "        #------------------计算6 Omega, 也就是时间权重 omega越小，越重要-----------\n",
    "\n",
    "\n",
    "        #------------------Calculating sub-model weights-----------\n",
    "        # Calculer les poids des sous-modèles\n",
    "        weight_list = []\n",
    "        for key,value in dic_expert.items(): \n",
    "            denominator = 0 \n",
    "            for j in range(0, myFeeder.t - value[1] +1): #  from 0 to  t-k\n",
    "                if ((myFeeder.t-j) in dic_expert.keys()):\n",
    "                    denominator += dic_expert[myFeeder.t-j][6]*dic_expert[myFeeder.t-j][5]+0.1  # Omega 权重 * 5 当前块被加权的错误    \n",
    "            value[2] = math.log(1/denominator)  \n",
    "            weight_list.append(math.log(1/denominator)) \n",
    "        for key,value in dic_expert.items(): \n",
    "            value[2] = (value[2] - min(weight_list)+0.1)/(max(weight_list) - min(weight_list)+0.1)\n",
    "        for key,value in dic_expert.items(): \n",
    "            print('key ', key, 'value[2] ',value[2] )\n",
    "        #------------------Calculating sub-model weights-----------\n",
    "\n",
    "\n",
    "        #------------------Removing the worst model -----------\n",
    "        if len(dic_expert) > maxNumSousModle:\n",
    "            maxErr = 0\n",
    "            del_key = 0\n",
    "            for key,value in dic_expert.items(): \n",
    "                if dic_expert[key][7] > maxErr:\n",
    "                    maxErr = dic_expert[key][7]\n",
    "                    del_key = key\n",
    "            dic_expert.pop(del_key)\n",
    "        #------------------去掉模型 最差策略 Sous-modèle de suppression de la pire stratégie-----------\n",
    "        \n",
    "        \n",
    "        #------------------Whether the sub-model LSTM is updated-----------\n",
    "        if updateOrNon:\n",
    "            updatingALLSousModele(dic_expert,actul_Batch_X,actul_Batch_Y) \n",
    "        #------------------子模型LSTM是否更新-----------\n",
    "\n",
    "\n",
    "        # 记录时间 Durée d'enregistrement\n",
    "        timeList.append(round(time.process_time()-start,3)) #记录时间\n",
    "\n",
    "        print('Average errors so far np.mean(errList)', np.mean(errList))    #展示平均\n",
    "        print('Current Error err_H', err_H)\n",
    "\n",
    "        #进入下一块\n",
    "        print('myFeeder.t ', myFeeder.t)\n",
    "        myFeeder.goNext()\n",
    "\n",
    "        # print(errList)\n",
    "\n",
    "    \n",
    "    patch = \"resulta/\"\n",
    "    # nameFileSave = \"CUBEM_resulta\"\n",
    "    nameFileSave = \"UKDALE_resulta\"\n",
    "    res_oneTour = {}\n",
    "    res_oneTour['Resulta_list'] =  Resulta_list\n",
    "    res_oneTour['Ture_list'] =  Ture_list\n",
    "    np.save(patch + nameFileSave + '.npy', res_oneTour , allow_pickle=True, fix_imports=True)\n",
    "\n",
    "    break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "662px",
    "left": "601px",
    "right": "20px",
    "top": "121px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
